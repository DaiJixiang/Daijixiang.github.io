<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>scrpy使用方法</title>
      <link href="/2019/02/01/scrapy%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/"/>
      <url>/2019/02/01/scrapy%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>Scrapy教程</p><p>将要抓取 quotes.toscrape.com网站。</p><p>此教程指导你完成以下任务：</p><p>新建Scrapy工程<br>编写一个spider爬网站提取数据<br>用命令行导出爬取的数据<br>改变spider递归爬行链接<br>使用spider参数<br>创建一个项目<br>在抓取之前，先要构建Scrapy项目：<br>scrapy startproject tutorial<br>此命令创建以下内容的tutorial目录：</p><p>tutorial/<br>    scrapy.cfg            # deploy configuration file</p><pre><code>tutorial/             # project&apos;s Python module, you&apos;ll import your code from here    __init__.py    items.py          # project items definition file    pipelines.py      # project pipelines file    settings.py       # project settings file    spiders/          # a directory where you&apos;ll later put your spiders        __init__.py</code></pre><p>第一个爬虫<br>Spider是定义为爬取网站信息的类。必须继承自scrapy.Spider，定义初始请求，如何选择页面爬取url，以及如何解析页面内容提取数据。</p><p>这是我们第一个Spider的代码，把它保存在tutorial/spiders目录的 quotes_spider.py 中文件。</p><p>import scrapy</p><p>class QuotesSpider(scrapy.Spider):<br>    name = “quotes”</p><pre><code>def start_requests(self):    urls = [        &apos;http://quotes.toscrape.com/page/1/&apos;,        &apos;http://quotes.toscrape.com/page/2/&apos;,    ]    for url in urls:        yield scrapy.Request(url=url, callback=self.parse)def parse(self, response):    page = response.url.split(&quot;/&quot;)[-2]    filename = &apos;quotes-%s.html&apos; % page    with open(filename, &apos;wb&apos;) as f:        f.write(response.body)    self.log(&apos;Saved file %s&apos; % filename)</code></pre><p>如上，spider继承自 scrapy.Spider<br>并且定义了一些属性和方法。</p><p>name:标识spider。在项目中必须是唯一的，不能给不同的spider设置相同的名称。<br>start_requests():必须返回一个请求的迭代（可以返回一个请求的列表或者写一个生成器函数），spider从这里开始爬去。子序列请求从这些初始的请求自动生成。<br>parse():在每个请求完成时回掉方法。response参数是TextResponse类的实例，包含页面内容和一些选择器等函数操作。<br>parse()函数通常解析html，把抓到的数据提取为dicts，随后查找新的URLS创建新的请求。</p><p>如何运行我们的蜘蛛<br>在项目的最顶层目录运行：</p><p>scrapy crawl quotes</p><p>这条命令执行我们刚添加的名为quotes的蜘蛛。它发送一些请求到quotes.toscrape.com。你将得到如下输出：</p><p>… (omitted for brevity)<br>2016-12-16 21:24:05 [scrapy.core.engine] INFO: Spider opened<br>2016-12-16 21:24:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)<br>2016-12-16 21:24:05 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023<br>2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (404) &lt;GET <a href="http://quotes.toscrape.com/robots.txt&gt;" target="_blank" rel="noopener">http://quotes.toscrape.com/robots.txt&gt;</a> (referer: None)<br>2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET <a href="http://quotes.toscrape.com/page/1/&gt;" target="_blank" rel="noopener">http://quotes.toscrape.com/page/1/&gt;</a> (referer: None)<br>2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET <a href="http://quotes.toscrape.com/page/2/&gt;" target="_blank" rel="noopener">http://quotes.toscrape.com/page/2/&gt;</a> (referer: None)<br>2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-1.html<br>2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-2.html<br>2016-12-16 21:24:05 [scrapy.core.engine] INFO: Closing spider (finished)<br>…<br>现在，检查当前目录。你会注意到创建了两个新文件quotes-1.html 和quotes-2.html，里面包含了urls的响应数据。</p><p>内部机制是什么<br>Scrapy调用蜘蛛的start_requests方法，一旦接收到一个响应，立马初始化Response对象然后调用请求的回掉函数（在此例中，时parse()函数）把response对象作为参数。</p><p>start_requests函数简写<br>作为start_requests函数的替代，可以定义start_urls的种子列表。默认的start_requests()函数实现中会使用start_urls创建初始请求。</p><p>import scrapy</p><p>class QuotesSpider(scrapy.Spider):<br>    name = “quotes”<br>    start_urls = [<br>        ‘<a href="http://quotes.toscrape.com/page/1/&#39;" target="_blank" rel="noopener">http://quotes.toscrape.com/page/1/&#39;</a>,<br>        ‘<a href="http://quotes.toscrape.com/page/2/&#39;" target="_blank" rel="noopener">http://quotes.toscrape.com/page/2/&#39;</a>,<br>    ]</p><pre><code>def parse(self, response):    page = response.url.split(&quot;/&quot;)[-2]    filename = &apos;quotes-%s.html&apos; % page    with open(filename, &apos;wb&apos;) as f:        f.write(response.body)</code></pre><p>urls的每次请求都会调用parse()。这是因为parse()是Scrapy在没有显式给回掉函数赋值时的默认回掉函数。</p><p>提取数据<br>最好的学习使用Scrapy的选择器的方式是使用Scrapy shell。</p><p>scrapy shell ‘<a href="http://quotes.toscrape.com/page/1/&#39;" target="_blank" rel="noopener">http://quotes.toscrape.com/page/1/&#39;</a></p><p>提示<br>记住使用单引号包裹地址否则包含参数（如&amp;字符）将不会工作</p><p>在windows中，使用双引号</p><p>你将看到：</p><p>[ … Scrapy log here … ]<br>2016-09-19 12:09:27 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET <a href="http://quotes.toscrape.com/page/1/&gt;" target="_blank" rel="noopener">http://quotes.toscrape.com/page/1/&gt;</a> (referer: None)<br>[s] Available Scrapy objects:<br>[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)<br>[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x7fa91d888c90&gt;<br>[s]   item       {}<br>[s]   request    &lt;GET <a href="http://quotes.toscrape.com/page/1/&gt;" target="_blank" rel="noopener">http://quotes.toscrape.com/page/1/&gt;</a><br>[s]   response   &lt;200 <a href="http://quotes.toscrape.com/page/1/&gt;" target="_blank" rel="noopener">http://quotes.toscrape.com/page/1/&gt;</a><br>[s]   settings   &lt;scrapy.settings.Settings object at 0x7fa91d888c10&gt;<br>[s]   spider     <defaultspider 'default'="" at="" 0x7fa91c8af990=""><br>[s] Useful shortcuts:<br>[s]   shelp()           Shell help (print this help)<br>[s]   fetch(req_or_url) Fetch request (or URL) and update local objects<br>[s]   view(response)    View response in a browser</defaultspider></p><blockquote><blockquote><blockquote><p>在shell中，可以使用response对象的CSS 函数选择元素。</p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>response.css(‘title’)<br>[<selector xpath="descendant-or-self::title" data="<title>Quotes to Scrape</title>">]<br>response.css(‘title’)的运行结果是一个名为SelectorList的list-like对象，它是包含XML/HTML元素的 Selector<br>对象列表允许你进一步查询选择和提取数据。</selector></p></blockquote></blockquote></blockquote><p>为了导出title的文本，你可以：</p><blockquote><blockquote><blockquote><p>response.css(‘title::text’).extract()<br>[‘Quotes to Scrape’]<br>此处有两点要注意：一、我们添加了::text到CSS查询中，意味着只选择了<title>元素的text属性。如果不指定::text，我们会得到的是整个title元素。</title></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>response.css(‘title’).extract()<br>[‘<title>Quotes to Scrape</title>‘]<br>二、.extract()返回SelectorList的文本列表。只取第一个元素的文本：</p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>response.css(‘title::text’).extract_first()<br>‘Quotes to Scrape’<br>也可以使用python的列表写法：</p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>response.css(‘title::text’)[0].extract()<br>‘Quotes to Scrape’<br>但是，使用extract()和extract_first()方法可以在没有找到任何匹配元素时返回None，避免IndexError。</p></blockquote></blockquote></blockquote><p>除了extract()和extract_first()方法，你还可以使用re()的正则表达式方法。</p><blockquote><blockquote><blockquote><p>response.css(‘title::text’).re(r’Quotes.*’)<br>[‘Quotes to Scrape’]<br>response.css(‘title::text’).re(r’Q\w+’)<br>[‘Quotes’]<br>response.css(‘title::text’).re(r’(\w+) to (\w+)’)<br>[‘Quotes’, ‘Scrape’]<br>为了找到适当的CSS选择器，你可从shell中使用view(response)浏览响应界面。你可以使用浏览器开发工具或插件如Firebug（此处请看使用Firebug 抓取和使用FireFox抓取）。</p></blockquote></blockquote></blockquote><p>选择器小工具也是一个查找CSS选择器很好的工具，可以可视化的查找元素，可在很多浏览器中工作。</p><p>XPATH:简介<br>除了css，Scrapy选择器也支持XPath表达式：</p><blockquote><blockquote><blockquote><p>response.xpath(‘//title’)<br>[<selector xpath="//title" data="<title>Quotes to Scrape</title>">]<br>response.xpath(‘//title/text()’).extract_first()<br>‘Quotes to Scrape’<br>XPATH表达式很强大，是Scrapy选择器的基础。事实上，CSS选择器在内部转换为Xpath。你可以在shell查看文本选择器的对象类型。</selector></p></blockquote></blockquote></blockquote><p>尽管不如CSS选择器流行，Xpath表达式却更强大。它除了可以导航到结构也可以查找内容。使用xpath，你能这么选择如：选择包含Next Page的文本连接。这使得xpath非常适合抓取，我们鼓励你学习Xpath，即使你已经知道如何构造CSS选择器，它会更简单。</p><p>我们在这不会涉及XPath太多，你可以阅读使用XPath.为了学习Xpath，我们建议通过例子学习XPath教程，和如何使用XPath思考。</p><p>提取quotes和authors<br>现在你知道了一点关于选择和提取的知识了，让我们完善我们的spider，写代码从网站页面提取quotes。</p><p><a href="http://quotes.toscrape.com中的每个quote的HTML形式类似下面：" target="_blank" rel="noopener">http://quotes.toscrape.com中的每个quote的HTML形式类似下面：</a></p><p><div class="quote"><br>    <span class="text">“The world as we have created it is a process of our<br>    thinking. It cannot be changed without changing our thinking.”</span><br>    <span><br>        by <small class="author">Albert Einstein</small><br>        <a href="/author/Albert-Einstein">(about)</a><br>    </span><br>    <div class="tags"><br>        Tags:<br>        <a class="tag" href="/tag/change/page/1/">change</a><br>        <a class="tag" href="/tag/deep-thoughts/page/1/">deep-thoughts</a><br>        <a class="tag" href="/tag/thinking/page/1/">thinking</a><br>        <a class="tag" href="/tag/world/page/1/">world</a><br>    </div><br></div><br>打开scrapy shell尝试提取我们想要的数据。</p><p>$ scrapy shell ‘<a href="http://quotes.toscrape.com&#39;" target="_blank" rel="noopener">http://quotes.toscrape.com&#39;</a><br>我们使用下面语法得到一系列的quote元素的选择器：</p><blockquote><blockquote><blockquote><p>response.css(“div.quote”)<br>每个选择器都可以查询它们的子元素。我们把第一个选择器赋值给变量，这样我们可以直接运行指定的quote选择器。</p></blockquote></blockquote></blockquote><p>quote = response.css(“div.quote”)[0]<br>现在我们从quote导出title,author和tags使用我们刚创建的quote对象。当你知道你只需要第一个结果时，你可以：</p><blockquote><blockquote><blockquote><p>title = quote.css(“span.text::text”).extract_first()<br>title<br>‘“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”’<br>author = quote.css(“small.author::text”).extract_first()<br>author<br>‘Albert Einstein’<br>考虑到标签是字符串列表，我们可以使用.extract()方法获取他们。</p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>tags = quote.css(“div.tags a.tag::text”).extract()<br>tags<br>[‘change’, ‘deep-thoughts’, ‘thinking’, ‘world’]<br>解决了如何导出每个，我们现在可迭代所有quotes元素把他们保存到Python字典中。</p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>for quote in response.css(“div.quote”):<br>…     text = quote.css(“span.text::text”).extract_first()<br>…     author = quote.css(“small.author::text”).extract_first()<br>…     tags = quote.css(“div.tags a.tag::text”).extract()<br>…     print(dict(text=text, author=author, tags=tags))<br>{‘tags’: [‘change’, ‘deep-thoughts’, ‘thinking’, ‘world’], ‘author’: ‘Albert Einstein’, ‘text’: ‘“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”’}<br>{‘tags’: [‘abilities’, ‘choices’], ‘author’: ‘J.K. Rowling’, ‘text’: ‘“It is our choices, Harry, that show what we truly are, far more than our abilities.”’}<br>    … a few more of these, omitted for brevity</p><p>使用spider导出数据<br>让我们回到spider。直到现在，仍然没有导出任何数据，只是把HTML页面保存到本地文件中。我们把导出逻辑集成到spider中。</p></blockquote></blockquote></blockquote><p>一个Scrapy蜘蛛通常包含多个页面抓取数据的字典。这样，我们可以使用在回调函数中使用yieldPython关键字，如下所示：</p><p>import scrapy</p><p>class QuotesSpider(scrapy.Spider):<br>    name = “quotes”<br>    start_urls = [<br>        ‘<a href="http://quotes.toscrape.com/page/1/&#39;" target="_blank" rel="noopener">http://quotes.toscrape.com/page/1/&#39;</a>,<br>        ‘<a href="http://quotes.toscrape.com/page/2/&#39;" target="_blank" rel="noopener">http://quotes.toscrape.com/page/2/&#39;</a>,<br>    ]</p><pre><code>def parse(self, response):    for quote in response.css(&apos;div.quote&apos;):        yield {            &apos;text&apos;: quote.css(&apos;span.text::text&apos;).extract_first(),            &apos;author&apos;: quote.css(&apos;span small::text&apos;).extract_first(),            &apos;tags&apos;: quote.css(&apos;div.tags a.tag::text&apos;).extract(),        }</code></pre><p>如果你运行这个蜘蛛，它把导出数据输出到日志中：</p><p>2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 <a href="http://quotes.toscrape.com/page/1/&gt;" target="_blank" rel="noopener">http://quotes.toscrape.com/page/1/&gt;</a><br>{‘tags’: [‘life’, ‘love’], ‘author’: ‘André Gide’, ‘text’: ‘“It is better to be hated for what you are than to be loved for what you are not.”’}<br>2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 <a href="http://quotes.toscrape.com/page/1/&gt;" target="_blank" rel="noopener">http://quotes.toscrape.com/page/1/&gt;</a><br>{‘tags’: [‘edison’, ‘failure’, ‘inspirational’, ‘paraphrased’], ‘author’: ‘Thomas A. Edison’, ‘text’: ““I have not failed. I’ve just found 10,000 ways that won’t work.””}<br>保存抓取到的数据<br>最简单的保存抓取数据是使用Feed exports, 使用下面的命令行：</p><p>scrapy crawl quotes -o quotes.json<br>这将生成一个quotes.json文件包含所有抓取像序列化为json。</p><p>由于历史原因，Scrapy使用追加而不是覆盖，如果你运行两次此命令而没有在第二次删除之前的文件，你将得到一个损毁的JSON文件。</p><p>你也可以使用其他格式，如Json Lines</p><p>scrapy crawl quotes -o quotes.jl<br>Json Lines格式很有用，因为她是stream-like。你可以往里面轻松的添加新纪录。他没有上面的JSON文件的问题当你运行两次的时候。同时，因为每条记录是一行，你可以处理超大文件而不必担心内存问题，有很多工具如JQ可在命令行处理。</p><p>在小项目里（例如此教程），这样就够了。然而，如果你想处理更复杂的抓取项，你可以编写[Item 管道]。当创建项目的时候，会在tutorial/pipelines.py构建一个Item 管道文件。这样如果你只是想保存抓取到的项，就不需要实现任何的Item管道。</p><p>下面的连接<br>假如你不仅想抓取 <a href="http://quotes.toscrape.com网站中的两个页面，而是想抓取所有的网站页面。" target="_blank" rel="noopener">http://quotes.toscrape.com网站中的两个页面，而是想抓取所有的网站页面。</a></p><p>现在你知道如何从页面抓取数据，让我们看看下面的连接如何得到。</p><p>首先从页面中提取我们想要的连接。查看我们的页面，我们可以看见页面中的下一页连接如下所示标志：</p><p><ul class="pager"><br>    <li class="next"><br>        <a href="/page/2/">Next <span aria-hidden="true">→</span></a><br>    </li><br></ul><br>试着在shell中提取它：</p><blockquote><blockquote><blockquote><p>response.css(‘li.next a’).extract_first()<br>‘<a href="/page/2/">Next <span aria-hidden="true">→</span></a>‘<br>这得到了整个anchor元素，但是我们想要href属性。为了如此，Scrapu提供了CSS的扩展使你可以选择属性内容，如下：</p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>response.css(‘li.next a::attr(href)’).extract_first()<br>‘/page/2/‘<br>现在我们的spider被改成了可以跟踪下一页从中导出数据：</p></blockquote></blockquote></blockquote><p>import scrapy</p><p>class QuotesSpider(scrapy.Spider):<br>    name = “quotes”<br>    start_urls = [<br>        ‘<a href="http://quotes.toscrape.com/page/1/&#39;" target="_blank" rel="noopener">http://quotes.toscrape.com/page/1/&#39;</a>,<br>    ]</p><pre><code>def parse(self, response):    for quote in response.css(&apos;div.quote&apos;):        yield {            &apos;text&apos;: quote.css(&apos;span.text::text&apos;).extract_first(),            &apos;author&apos;: quote.css(&apos;span small::text&apos;).extract_first(),            &apos;tags&apos;: quote.css(&apos;div.tags a.tag::text&apos;).extract(),        }    next_page = response.css(&apos;li.next a::attr(href)&apos;).extract_first()    if next_page is not None:        next_page = response.urljoin(next_page)        yield scrapy.Request(next_page, callback=self.parse)</code></pre><p>现在，导出数据后，parse()函数查找下一页，使用urljoin构建一个绝对路径URL并生成一个到下一页的新请求，把下一页的请求注册为回调使得蜘蛛可以爬到所有的页面。</p><p>这是Scrapy跟踪页面的机制：当你在回调中生成一个请求对象，Scrapy会安排请求发送并注册回调函数在请求结束时运行。</p><p>使用这些，你可以构建复杂的爬虫系统，链接规则可以自定义，根据访问页面导出各种各样的数据。</p><p>在我们的例子中，它创建了一系列循环跟踪所有的链接到下一页直到找不到任何连接——方便爬取博客，论坛或其他的导航网站。</p><p>更多示例和模式<br>这是另一个蜘蛛用来解释回调和跟踪连接，这次抓取作者信息：</p><p>import scrapy</p><p>class AuthorSpider(scrapy.Spider):<br>    name = ‘author’</p><pre><code>start_urls = [&apos;http://quotes.toscrape.com/&apos;]def parse(self, response):    # follow links to author pages    for href in response.css(&apos;.author+a::attr(href)&apos;).extract():        yield scrapy.Request(response.urljoin(href),                             callback=self.parse_author)    # follow pagination links    next_page = response.css(&apos;li.next a::attr(href)&apos;).extract_first()    if next_page is not None:        next_page = response.urljoin(next_page)        yield scrapy.Request(next_page, callback=self.parse)def parse_author(self, response):    def extract_with_css(query):        return response.css(query).extract_first().strip()    yield {        &apos;name&apos;: extract_with_css(&apos;h3.author-title::text&apos;),        &apos;birthdate&apos;: extract_with_css(&apos;.author-born-date::text&apos;),        &apos;bio&apos;: extract_with_css(&apos;.author-description::text&apos;),    }</code></pre><p>蜘蛛从主页面开始，使用parse_author回调函数跟踪所有的作者页面连接，同时用parse回调函数跟踪导航连接如我们之前看到的。</p><p>parse_author回调函数定义了一个帮助方法，从CSS查询提取和清理并使用作者数据生成Python dict。</p><p>另一件关于蜘蛛的有趣的事情是，即使有很多名言出自同一作者，我们也不必担心多次访问相同作者的页面。默认情况下，Scrapy过滤掉重复的已访问的请求地址，避免程序太多次点击服务器的问题。这是用DUPEFILTER_CLASS配置。</p><p>希望你已理解了Scrapy如何跟踪页面和回调的机制。</p><p>这个程序利用跟踪链接机制实现，查看CrawlSpider类,它是一个通用的蜘蛛实现了一个小的规则引擎，你可以在这之上编写自己的爬虫。</p>]]></content>
      
      
      <categories>
          
          <category> 爬虫 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>FirstPassage</title>
      <link href="/2019/01/30/FirstPassage/"/>
      <url>/2019/01/30/FirstPassage/</url>
      
        <content type="html"><![CDATA[<p>#<center> 这是第一篇博客文章</center><br>上世纪80年代，改革先锋项南去探望老帅聂荣臻。谈话间，聂帅跟他提起河北阜平县，一座当年晋察冀边区的革命老城。聂帅说：“老百姓保护了我们、养育了我们，我们打下了天下，是为老百姓打下的天下，阜平的乡亲们现在生活还没有明显改善，我于心不忍，一定要把老区的事情办好。”<a id="more"></a></p><p>老帅话里有故事。战争时期，阜平人口不足9万，养活了超过9万人的部队。超过1/5的阜平人参军参战，5000多人再没回来。</p><p><strong>“</strong> 阜平不富，死不瞑目。<strong>”</strong>说这话时，老帅流泪了。</p><p>这个故事是项南讲给习近平听的，习近平深受触动：<strong>“我们对脱贫攻坚特别是老区脱贫致富，要有一种责任感、紧迫感，要带着感情做这项工作。”</strong></p><p>2012年11月30日，距离北京300公里的阜平骆驼湾村，地处深山，气温零下十几度，雪还没消，贫困户唐荣斌家里的木门和上面贴的福字一样，被时光侵蚀得掉了色。门外寒气逼人，帘子一掀，走进来的是满脸笑容的习近平总书记。</p><p>此时，距离党的十八大闭幕刚刚两周。外头天冷，老唐两口子跟孙子在炕上摆了火盆，添了些炭。习近平盘腿坐上炕，跟老人拉起家常：一年挣多少钱，粮食够不够，生火的煤存了多少，小孩上学远不远。他说：“元旦快到了，我特意来看看大家。”</p><p>阜平是他担任总书记之后出京考察的第二站。第一站在深圳，他向邓小平铜像敬献花篮，向外界传递坚持改革开放的讯息；第二站就到了阜平。他说，没有农村的小康，特别是没有贫困地区的小康，就没有全面建成小康社会。他鼓励大家：<strong>“只要有信心，黄土变成金。”</strong></p>]]></content>
      
      
      <categories>
          
          <category> 练习一 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>你好，Hexo</title>
      <link href="/2019/01/30/%E4%BD%A0%E5%A5%BD%EF%BC%8CHexo/"/>
      <url>/2019/01/30/%E4%BD%A0%E5%A5%BD%EF%BC%8CHexo/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2019/01/30/hello-world/"/>
      <url>/2019/01/30/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
